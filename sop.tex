\documentclass[11pt]{article}

\usepackage{times}
\usepackage{xspace}
\usepackage[margin=1in]{geometry}

\pagenumbering{gobble}

\newcommand\lila{\textsc{L\={\i}la}\xspace}
\newcommand\inst{INSTITUTE\xspace}

\begin{document}

Pre-trained language models (LMs) with billions of parameters 
are the go-to models for many NLP tasks.
The sheer scale of their training corpora and parameter counts
appears to endow them with the ability to compete with, 
and frequently outperform, task-specific models.
LMs, however, can often have unexpected failure modes 
and are notoriously difficult to control.
The responsible refinement and improvement of these models
depends upon the development of the necessary tools 
and frameworks for understanding them.
While an undergraduate at Harvard and subsequently as a pre-doctoral researcher 
at AI2, I have developed an approach of \textbf{
  drawing on fields such as syntax and formal language theory 
  to develop practical and theoretical frameworks  
  towards understanding and evaluating language models 
  as general-purpose NLP systems.
}

\paragraph{Understanding LMs through linguistics}

Modern NLP has become largely divorced 
from our understanding of linguistics and cognition in humans.
Reconciling these two can lend clarity to our understanding of how LMs work.
As my first foray into this effort, I set out to 
discover how modern LMs handle syntactic agreement.
In my first-author ACL 2021 paper and oral presentation~\cite{Finlayson2021CausalAO}
we intervene on individual neurons in large transformers 
to observe their causal effect on syntactic agreement.
We find, among other things, that transformers learn 
two distinct mechanisms for number agreement,
and that these mechanisms are distributed in the MLP activations of the network.
This finding contrasts with earlier research that found that 
gender bias effects were concentrated in the attention heads of the models~\cite{Vig2020InvestigatingGB}.
Subsequent research has built on our work 
in order to analyze other linguistic phenomena 
such as distributivity~\cite{Ban2022TestingPL},
and the effect of relative clauses on agreement~\cite{Ravfogel2021CounterfactualIR}.
Studies like these lend important insights into how these models work internally.
During my PhD, I hope to explore the potential of this style of research 
to evaluate and improve the linguistic capabilities of neural networks. 
As an example, I am interested in leveraging the language acquisition literature
to evaluate and improve LMs' abilities 
to acquire new words by inferring their meaning from context.
This might be achieved via a self-supervised pre-training objective 
that randomly replaces lexical items with new unseen words, 
forcing the model to learn how to generalize effectively.
Success here would result in LMs that are 
more robust to linguistic distribution shifts
and adapt to evolving language.

\paragraph{Understanding LMs through formal languages}

I am excited by projects 
that borrow from formal language theory
to increase our understanding of LMs.
This approach makes it possible to answer 
questions that might otherwise be very difficult to approach using natural language.
For instance, I used regular languages
to measure the capabilities of 
\textbf{transformers as general instruction followers}
in RegSet, my first-author EMNLP 2022 paper 
and oral presentation~\cite{Finlayson2022WhatMI}. 
Large, pre-trained LMs can solve some NLP tasks 
by conditioning their generations on natural language instructions 
for the task~\cite{mishra2021crosstask, Wei2021FinetunedLM}. 
However, the complexity of natural language makes it difficult to
predict what types of instructions may be challenging for transformers.
To solve this predicament, 
we propose a controllable proxy for studying instruction learning
by studying LMs' ability to follow instructions in the form of regular expressions.
We test the effects of attributes of regular languages,
such starfreeness, on their difficulty as instructions.
Our experiments lead us to a number of intriguing hypotheses 
about what makes instruction learning hard, 
including evidence that even large transformers struggle with modular counting 
(e.g., determining whether something is even or odd). 
By taking advantage of the well studied attributes of formal languages,
we achieved fine-grained control over our data, leading to findings that
would have been extremely difficult and expensive to obtain on natural data.
This approach can be applied more broadly to develop benchmarks 
that isolate and measure progress towards specific abilities 
in transformers that we might hope to see in natural language settings.

On the theoretical side, I am currently developing a framework for 
understanding what transformers can learn from instructions. 
This builds on prior work~\cite{Merrill2022LogPrecisionTA}
that achieves an initial lower bound for this problem.
In my PhD, I hope to derive additional theoretical results that help us
understand modern neural architectures through formal languages.
For instance, recent work~\cite{Wies2022SubTaskDE} 
proves that subtask decomposition,
in the style of chain-of-thought reasoning~\cite{Wei2022ChainOT},
enables learning difficult sequence-to-sequence tasks.
I would be interested in extending this work 
to characterize the additional computational power 
afforded to transformers via subtask decomposition.
These types of results are not only intellectually interesting,
but also provide both a principled way to study transformers 
and bounds on what we can expect them to learn.

\paragraph{Generalization}

My work on RegSet bridges to another area of research I hope to continue in during my PhD: generalization.
Research has shown that neural models consistently to struggle with
compositional generalization~\cite{Lake2018GeneralizationWS}. 
This bodes poorly for models in succeeding on tasks 
like the instruction following regime I studied with RegSet 
where the space of task descriptions is both intractably large 
and highly compositional.
I am interested in studying and developing models and methods
that tackle these types of issues.

Some of my past and current work deals with generalization 
by attempting to develop methods for utilizing pre-trained language models
to their full capacity as general problem solvers.
In a preprint currently under submission to ICLR 2023~\cite{Khot2022DecomposedPA},
I develop a modular prompting method for recursively prompting large LMs 
in order to vastly improve length generalization 
compared to few-shot and step-by-step reasoning style prompting.
Currently, I am developing methods for using language models 
to decode their own task-specific prompts.
One exciting application of this method would be to explore 
the possibility of adversarial prompts: 
prompts that appear to elicit one behavior 
but cause the model to exhibit another.
Research into adversarial prompts 
is an important step towards to developing 
secure and ethical general-purpose NLP systems. 

A crucial part of developing general-purpose systems is evaluating them.
As an example, 
current evaluation schemes fail to holistically evaluate 
general-purpose math reasoning skills in LMs
because they are far too narrow in scope.
Towards improving evaluation in this area, 
I led a team of 11 researchers in compiling a
\textbf{comprehensive and diverse natural language math reasoning benchmark.} 
I introduce the benchmark, \lila~\cite{Mishra2022Lila}, 
in my first-author EMNLP 2022 paper
where we draw together a diverse set over 140K math problems.
We additionally provide valuable annotations for mathematical reasoning via program synthesis, 
and show that multitask learning, 
combined with augmenting the model with a Python interpreter,
massively improves LMs ability to do math reasoning 
with explicit reasoning steps.
Our publicly released multitask model, christened Bh\=askara, 
outperforms similarly-sized T5 and GPT-Neo models
when fine-tuning on new mathematical reasoning tasks.
\lila shows that LMs 
in their current form 
are woefully deficient when it comes to math reasoning,
and highlights the need for these kinds of unified evaluations for 
aspiring general-purpose math reasoning models.
During my PhD I hope to continue to develop thoughtful and comprehensive evaluations 
to measure and promote research into models with greater general utility.

\paragraph{Future plans}
After my PhD and postdoc I hope to become a PI 
at an institution where I can achieve autonomy in choosing my research directions
while also pursuing my passion for mentorship teaching.
Ideally this means a professorship at a research university.
I value autonomy in research because I work best 
when I can focus my energy on projects 
that are intellectually interesting to me.
I have also especially become aware of the importance of mentorship 
throughout my undergrad and time at AI2.
I am passionate about promoting access to mentorship 
as a way to level the playing field 
for the next generation of researchers.

\paragraph{Fit for \inst} I am specifically interested in joining the CS program at 
\inst because\ldots Professor NAME's work on\ldots is particularly interesting to me given my interest in\ldots

\bibliography{ref}
\bibliographystyle{unsrt}

\end{document}

