\documentclass[11pt]{article}

\usepackage{times}
\usepackage{xspace}
\usepackage[margin=1in]{geometry}

\pagenumbering{gobble}

\newcommand\lila{\textsc{L\={\i}la}\xspace}
\newcommand\inst{Cornell\xspace}

\begin{document}

Pre-trained language models (LMs) with billions of parameters 
have proven their versatility 
as general-purpose NLP systems. 
By general-purpose, I mean that a single, frozen LM can be used 
in lieu of a task-specific architecture or fine-tuned model 
for a multitude of tasks.
However, these models have unexpected failure modes 
and are notoriously difficult to control.
Responsibly developing these models
requires proper tools for understanding them.
As an undergraduate computational linguistics researcher at Harvard 
(advised by Stuart Shieber and Yonatan Belinkov)
and subsequently as a pre-doctoral researcher at AI2 
(advised by Peter Clark and Ashish Sabharwal)
I have cultivated a multi-perspective approach to understanding and using LMs.
I aim to build on this work during my PhD
by drawing on fields such as syntax and formal language theory 
to create practical and theoretical frameworks  
towards \textbf{understanding, evaluating, and safely using} language models 
as \textbf{general-purpose NLP systems.}

\paragraph{Understanding and improving LMs through linguistics}

Modern NLP is largely divorced 
from our understanding of linguistics and cognition in humans.
Reconciling these two can lend clarity to our understanding of how LMs work, 
which in turn is the first step towards improving them.
As my first foray into this effort, I set out to 
discover how modern LMs handle syntactic agreement.
In my first-author ACL 2021 paper~\cite{finlayson-etal-2021-causal}
we intervene on individual neurons in large transformers 
to observe their causal effect on syntactic agreement.
We find, among other things, that transformers learn 
two distinct mechanisms for number agreement,
and that these mechanisms are distributed 
in the activations of the network, 
rather than concentrated in any single model component,
as was found with gender bias~\cite{Vig2020InvestigatingGB}.
These findings constitute a step forward 
in understanding and interpreting
how LMs mimic syntactic behavior,
shedding light into the black box model.
Subsequent research has built on our work 
in order to interpret other linguistic phenomena 
such as distributivity~\cite{Ban2022TestingPL}.
In future research, I hope to explore 
linguistically informed approaches
to evaluating and improving the capabilities of LMs. 
As an example, I am interested in leveraging 
the language acquisition literature
to evaluate and improve LMs' abilities 
to acquire new words by inferring their meaning from context.
Improving this ability might be achieved 
via a self-supervised pre-training objective 
that randomly replaces lexical items with new unseen words, 
forcing the model to learn how to generalize effectively.
Success here would result in LMs that are 
more robust to linguistic distribution shifts
and adapt to evolving language.

\paragraph{Understanding LMs through formal languages}

I am excited by projects 
that borrow from formal language theory
to increase our understanding of LMs.
This approach makes it possible to answer 
questions that might otherwise be very difficult to study via natural language.
For instance, I used regular languages to 
\textbf{measure the capabilities of transformers as instruction followers}
in RegSet, my first-author EMNLP 2022 paper~\cite{Finlayson2022WhatMI}. 
Large, pre-trained LMs can solve some NLP tasks 
by conditioning their generations on natural language instructions 
for the task~\cite{mishra2021crosstask, Wei2021FinetunedLM}. 
On the other hand, 
research has shown that neural models consistently struggle with
compositional generalization~\cite{Lake2018GeneralizationWS}. 
This bodes poorly for the instruction following regime
where the space of task descriptions is both intractably large 
and highly compositional.
Moreover, the fuzziness of natural language makes it difficult to
predict what types of instructions may be challenging for transformers.
To solve this predicament, 
we propose a controllable proxy for studying instruction learning
by studying LMs' ability to follow instructions in the form of regular expressions.
We test the effects of attributes of regular languages,
such as starfreeness~\cite{Salomaa1981JewelsOF}, on their difficulty as instructions.
Our experiments lead us to a number of intriguing hypotheses 
about what makes instruction learning hard, 
including evidence that even large transformers struggle with modular counting 
(e.g., determining whether something is even or odd). 
By taking advantage of the well studied attributes of formal languages,
we achieve fine-grained control over our data, leading to findings that
would have been extremely difficult and expensive to obtain on natural data.
This approach can be applied more broadly to develop benchmarks 
that isolate and measure progress towards specific abilities 
in transformers that we might hope to see in natural language settings.

Formal language theory can also be used to derive theoretical results 
that help us understand modern neural architectures. 
I am currently developing a framework for 
understanding what transformers can learn from instructions. 
In particular, I hope to show \textbf{which formal language families 
are provably learnable by transformers 
via instructions.}
This builds on prior work~\cite{Merrill2022LogPrecisionTA},
and provides both a principled way to study transformers 
and bounds on what we can expect them to learn.
Another useful direction could be to study
\textbf{the theoretical implications of sub-task decomposition.}
I have previously worked on empirical studies in this area:
in a preprint currently under submission 
to ICLR 2023~\cite{Khot2022DecomposedPA},
I implement a modular method for recursively prompting large LMs 
which vastly improves generalization to longer sequence lengths 
for certain types of tasks 
when compared to other step-by-step reasoning styles.
Recent work~\cite{Wies2022SubTaskDE} 
proves that subtask decomposition
via step-by-step reasoning
enables learning difficult sequence-to-sequence tasks.
Extending this work
by characterizing the additional computational power 
these techniques afford transformers
would be help explain why step-by-step reasoning 
has emerged as such an effective strategy for prompting LMs.

\paragraph{Evaluating and mitigating risks from general-purpose LMs}
Proper evaluation is critical to advancing general-purpose NLP systems,
and current methods are often insufficient for this purpose.
For instance, existing datasets for are too narrow in scope 
to holistically evaluate general-purpose math reasoning skills in LMs.
To address this,
I led a team of 11 researchers in compiling a
comprehensive and diverse natural language 
\textbf{math reasoning benchmark} 
called \lila~\cite{Mishra2022LilaAU} (first-author paper, EMNLP 2022).
We curate over 140K math problems
and provide valuable annotations for reasoning via program synthesis. 
Our experiments show that multitask learning 
and augmenting the model with a Python interpreter
massively improves LM performance 
while also providing explicit reasoning steps in the form of generated programs.
Our multitask model, Bh\=askara, 
outperforms similarly-sized models
when fine-tuning on new math reasoning tasks.
Despite our modeling contributions, \lila also shows that LMs 
in their current form 
are woefully deficient when it comes to math reasoning,
and highlights the need for unified evaluations for 
aspiring general-purpose reasoning models.
I plan to continue to create thoughtful and comprehensive evaluations 
to measure and promote research into models with greater general utility.

During my PhD I also hope to expand research on the risks associated 
with general-purpose NLP systems and how to mitigate them.
For instance, I am currently developing a decoding procedure 
for using frozen LMs as black boxes
to generate their own task-specific prompts.
I hope to apply this technique
to study \textbf{adversarial prompts:}
prompts that appear to elicit one behavior 
but cause the model to exhibit another.
These prompts could be generated by simultaneously 
decoding for fluency for one task and accuracy on another.
Exposing these types of vulnerabilities
enables the research community to better achieve
secure and ethical general-purpose NLP systems. 

\paragraph{At \inst} 

I am especially interested in working 
with Professors Alexander Rush and Yoav Artzi,
based on our overlap in research interests. 
In particular, I would be excited to work with Dr.\ Rush 
on building controllable NLP systems 
by developing our theoretical understanding of generative models 
and applying this knowledge to build principled controls to improve them. 
I also find Dr.\ Artzi's approach to research compelling, 
and I would be particularly interested in working together 
on building NLP systems that adapt and generalize in on-line settings
and novel contexts, especially by drawing on our linguistic understanding of pragmatics and linguistic change.
I would also be excited to work with other faculty in Cornell's NLP group
and collaborate with the NLP-adjacent groups such as the CLab and C.Psyd.


\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}

