\documentclass[12pt]{article}

\usepackage{times}
\usepackage[margin=1in]{geometry}

\newcommand\lila{\textsc{L\={\i}la}}

\begin{document}

I first took an interest in computational linguistics 
while learning to speak Tagalog. 
Through the process, I learned a deep appreciation for the language's rich morphology,
and the immense complexity of language in general.
My enthusiasm for language has not diminished since that time
and I am excited to pursue a PhD in natural language processing (NLP).

Though some subfields of NLP research 
have advanced by leaps and bounds in the past few years,
our understanding of how these advances 
have come about remains unsatisfactory.
While transformers may get impressive results on, e.g., question answering,
we do not know what the fundamental limits 
of what current architectures can compute or approximate,
or what these models are doing internally.
Do they learn syntactic rules? 
Can they generalize to new problems composed of known concepts?
In my PhD I hope to answer some of these questions
by \textbf{drawing on fields such as formal language theory and syntax
to develop practical and theoretical frameworks 
for understanding modern NLP methods}.

As my first step towards this goal, 
I set out to \textbf{interpret how modern language models handle syntactic agreement}
while an undergrad at Harvard, advised by Stuart Shieber and Yonatan Belinkov.
I designed and ran experiments
to intervene on individual neurons in large transformers 
to observe their causal effect on syntactic agreement.
After running the experiments and obtaining results,
we discovered that another group 
(Aaron Mueller advised by Sebastian Gehrmann and Tal Linzen)
had independently ran very similar experiments and obtained almost identical results.
In our co-authored ACL 2021 paper~\cite{Finlayson2021CausalAO}
(oral presentation)
we apply causal analysis to understand 
how transformers handle number agreement between nouns and verbs. 
We find that transformers generally learn two distinct mechanisms
for agreement, depending on whether the relevant tokens are adjacent or not.
Subsequent work has used causal analysis 
for analyzing other linguistic phenomena 
such as distributivity~\cite{Ban2022TestingPL},
and the effect of relative clauses on agreement~\cite{Ravfogel2021CounterfactualIR}.
Strengthening our understanding of how 
(and whether) 
language models deal with well-studied linguistic phenomena
can give insight into what inductive biases these models learn, 
and can guide research towards methods for improvement.

Excited by our findings,
and starting as a pre-doctoral researcher at the Allen Institute for AI (AI2),
I decided to next explore how formal language theory 
can increase our understanding of the capabilities of transformer models.
In particular, I found formal language to be an excellent test-bed for
studying how well language models 
\textbf{generalize on highly compositional tasks}. 
Compositional generalization is one area in which 
neural methods have been shown 
to consistently underperform humans~\cite{Lake2018GeneralizationWS}.
This becomes particularly problematic 
as \textbf{instruction following} emerges 
as a prominent paradigm~\cite{mishra2021crosstask, Wei2021FinetunedLM}
for building general-purpose large language models. 
Instruction following is where a language model is expected to perform a novel task
(one not seen in training) given only a description of the task.
Since this paradigm has emerged relatively recently
little is known about what kinds of tasks current models can be expected to learn.
To evaluate this and provide a synthetic sandbox for future research,
I introduce RegSet~\cite{Finlayson2022WhatMI} in my first-author EMNLP 2022 paper 
(oral presentation) 
advised by Kyle Richardson, Ashish Sabharwal and Peter Clark. 
In our work we propose a highly controllable proxy for studying instruction learning
by studying a language models ability to learn to interpret regular expressions.
Regular expressions are succinct representations of regular languages, 
a well studied and highly compositional class of formal languages.
As a result of our experiments,
we develop a handful of intriguing hypotheses 
about what makes instruction learning hard, 
including evidence that even large transformers struggle with modular counting 
(e.g., determining whether something is even or odd), 
less precise instructions, and tracking long contexts. 
We also release our challenging dataset, RegSet, to the public for further research.

Subsequent work~\cite{Merrill2022LogPrecisionTA} 
has built on our ideas by using formal language and computation theory
to formally characterize transformers ability to learn from instructions.
I am currently working with Ashish Sabharwal 
to further expand this framework.
My hope is that a better theoretical understanding 
of the limitations of transformers will inform 
our understanding of the architectural requirements 
for modeling language.

While we uncovering shortcomings of neural networks 
as general instruction followers,
I became interested in also evaluating neural networks as 
\textbf{general-purpose math reasoners}.
To accomplish this,
I led a team of 11 researchers in compiling a
\textbf{comprehensive and diverse natural language math reasoning benchmark.} 
I introduce the benchmark, \lila~\cite{Mishra2022Lila}, 
in my EMNLP 2022 paper (first-co-authored with Swaroop Mishra and advised by Ashwin Kalyan).
Current evaluation schemes fail to holistically evaluate 
general-purpose math reasoning skills in language models
because they are far too narrow in scope. 
As a result, they often overestimate the ability 
of particular models optimized for a single type of math reasoning.
In our large scale effort we draw together a diverse set of mathematical tasks 
and unite them under a single benchmark of over 140K math problems.
We provide valuable annotations for mathematical reasoning via program synthesis, 
where the language model has access to a Python interpreter.
Our experiments show that multi-task learning, 
combined with augmenting the model with a Python interpreter,
improves general-purpose math reasoning
and the resulting model is an effective starting point 
for downstream fine-tuning. 
At the same time, our benchmark shows language models, 
in their current form, 
are woefully deficient when it comes to math reasoning.
I led and contributed to all aspects of the \lila paper,
including collecting and annotating the datasets,
the experimental design, running the experiments, and writing the paper.
My hope is that our high quality, comprehensive evaluation  
will serve to unify evaluation
and further the effort towards developing general-purpose math reasoning models.

Building off of my work 
on learning from instructions and general-purpose reasoning, 
I am currently interested in
\textbf{learning how to leverage and control language models beyond fine-tuning}.
In our preprint~\cite{Khot2022DecomposedPA}, working closely with Tushar Khot and advised by Ashish Sabharwal, we develop methods for using large language models 
as problem decomposers and modular sub-problem solvers to improve performance over 
other prompting techniques such as chain-of-thought~\cite{}. 
In a current project, 
I am developing a decoding method for using a language model 
to self-generate optimal task-specific prompts 
relying only on inference-time sequence probability estimates from the model.

From these research experiences I have found that 
I am interested in pursuing these types of problems as a PhD student:
\begin{itemize}
  \item Tackling problems that deal with generality,
    from compositional generalization, to general purpose reasoning.
  \item Improving our fundamental understanding of our methods 
    rather than merely improving performance.
  \item Pushing beyond the task-specific fine-tuning paradigm
    by exploring new ways to leverage existing general-purpose models.
\end{itemize}

Eventually, I hope to become a PI 
at an institution where I can continue to pursue my passion 
for mentorship and teaching.
I have thoroughly enjoyed my teaching experiences as an undergrad 
and look forward to continuing to develop these skills as a PhD.


\bibliography{ref}
\bibliographystyle{unsrt}

\end{document}

