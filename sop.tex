\documentclass[11pt]{article}

\usepackage{times}
\usepackage{xspace}
\usepackage[margin=1in]{geometry}

\pagenumbering{gobble}

\newcommand\lila{\textsc{L\={\i}la}\xspace}
\newcommand\inst{Cornell\xspace}

\begin{document}

Pretrained language models (LMs) with billions of parameters 
have proven their versatility 
as general-purpose NLP systems, 
meaning that they can often
outperform task-specific models. 
However, they can have unexpected failure modes 
and are notoriously difficult to control.
Responsibly developing general-purpose LMs
requires creating tools for understanding them.
As an undergraduate computational linguistics researcher at Harvard 
(advised by Stuart Shieber and Yonatan Belinkov)
and subsequently as a pre-doctoral researcher at AI2 
(advised by Peter Clark and Ashish Sabharwal)
I cultivated a multi-perspective approach to understanding and using LMs.
I aim to build on this during my PhD
by drawing on fields such as syntax and formal language theory 
to create practical and theoretical frameworks  
towards \textbf{understanding, evaluating, and safely using} language models 
as \textbf{general-purpose NLP systems.}

\paragraph{Understanding and improving LMs through linguistics}

Modern NLP is largely divorced 
from our understanding of linguistics.
Reconciling these can lend clarity to how LMs work, 
which is a first step towards improving them.
As my first foray into research, I set out to 
discover how modern LMs handle syntactic agreement.
In my first-author ACL 2021 paper~\cite{finlayson-etal-2021-causal}
we intervene on neurons in transformers 
to observe their causal effect.
We find, among other things, that transformers learn 
two distinct mechanisms for number agreement,
and that these mechanisms are distributed 
in the activations of the network, 
rather than concentrated in any single model component,
as was found with gender bias~\cite{Vig2020InvestigatingGB}.
These findings elucidate
how LMs mimic syntactic behavior,
and subsequent research has built on our work 
to interpret other linguistic phenomena 
such as distributivity~\cite{Ban2022TestingPL}.
In future research, I hope to explore 
linguistically informed approaches
to evaluate and improve LM capabilities. 
E.g., I am interested in leveraging 
the language acquisition literature
to evaluate and improve LMs' abilities 
to acquire new words by inferring their meaning from context.
Improving this ability might be achieved 
via a pretraining on augmented data
that introduces hypothetical but plausible linguistic changes
forcing the model to learn how to generalize effectively.
Success would result in LMs that 
adapt to evolving language.

\paragraph{Understanding LMs through formal languages}

I am excited by projects 
that borrow from formal language theory
to increase our understanding of LMs.
This approach enables answering 
questions that we cannot study via natural language.
For instance, I used regular languages to 
\textbf{measure the capabilities of transformers as instruction followers}
in RegSet, my first-author EMNLP 2022 paper~\cite{Finlayson2022WhatMI}. 
Large, pretrained LMs solve some NLP tasks 
by conditioning their generations on natural language instructions 
for the task~\cite{mishra2021crosstask, Wei2021FinetunedLM}. 
On the other hand, LMs often struggle with
compositional generalization~\cite{Lake2018GeneralizationWS}. 
This bodes poorly for the instruction following regime
where the space of instructions is highly compositional.
Moreover, natural language fuzziness complicates predicting
which instructions are challenging for transformers.
In response, we propose a proxy for instruction learning
by studying instructions in the form of regular expressions.
We test the effects of regular language attributes
such as star-freeness~\cite{Salomaa1981JewelsOF}
on their difficulty as instructions.
Our experiments yield multiple hypotheses 
for what makes instruction learning hard, 
including evidence that even large transformers 
struggle with modular counting 
(e.g., distinguishing even from odd). 
Here, well-studied attributes of formal languages afford us 
fine-grained control over the data, 
precipitating findings that would be difficult and expensive 
to obtain on natural data.
I hope to apply this approach more broadly 
to isolate and measure progress towards other desirable abilities in LMs. 

We can also use formal language theory to derive theoretical results 
that help us understand neural LMs. 
I am currently developing a framework for 
proving \textbf{which formal language families 
are learnable by transformers via instructions.}
This builds on prior work~\cite{Merrill2022LogPrecisionTA},
and will hopefully provide bounds 
on what we can expect transformers to learn from instructions.
In the future, I would also like to study
\textbf{the theoretical implications of sub-task decomposition.}
I have previously worked on empirical studies in this area:
in my ICLR 2023 submission~\cite{Khot2022DecomposedPA}
I implement a modular, recursive LM prompting method
which vastly improves generalization to longer sequence lengths 
compared to other step-by-step reasoning styles.
In the future, I am interested in understanding why these methods work 
by formally characterizing the additional computational power 
that intermediate reasoning affords transformers.

\paragraph{Evaluating and mitigating risks from general-purpose LMs}

Comprehensive evaluation is critical to advancing general-purpose NLP systems.
For instance, existing datasets are too narrow in scope 
to holistically evaluate math reasoning skills in LMs.
To address this, I led 11 researchers 
in compiling a diverse natural language 
\textbf{math reasoning benchmark}, 
\lila~\cite{Mishra2022LilaAU} (first-author, EMNLP 2022).
We curate over 140K math problems
with annotations for reasoning via program synthesis. 
Our experiments show that multitask learning 
and augmenting with a Python interpreter
massively improve LM performance. 
Despite our modeling contributions, \lila shows that LMs 
are woefully deficient at math reasoning,
and demonstrates the need for unified evaluations of this sort.
Going forward, I plan to continue 
creating thoughtful, comprehensive benchmarks 
for general-purpose models.

During my PhD, I also hope to expand research 
on general-purpose NLP system vulnerabilities and how to mitigate them.
For instance, I am developing a decoding procedure where frozen LMs 
generate their own task-specific prompts.
I hope to apply this technique
to study \textbf{adversarial prompts}
that appear to elicit one behavior 
but cause the model to exhibit another.
These prompts could be generated by simultaneously 
decoding for fluency on one task and accuracy on another.
Exposing vulnerabilities
enables the research community to achieve
secure and ethical general-purpose NLP systems. 

\paragraph{At \inst} 

I am especially interested in working 
with Professors Alexander Rush and Yoav Artzi,
based on our overlap in research interests. 
In particular, I would be excited to work with Dr.\ Rush 
on building controllable NLP systems 
by developing our theoretical understanding of generative models 
and applying it to build principled controls to improve them. 
I also find Dr.\ Artzi's research approach compelling, 
and would be interested in working together 
to build NLP systems that adapt to novel contexts, 
especially by drawing on our understanding of pragmatics and linguistic change.
I would also be excited to work with other faculty in Cornell's NLP group
and collaborate with the NLP-adjacent groups such as CLab and C.Psyd.

\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}

