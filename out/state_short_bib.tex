[1] Ban, P., et al. Testing pre-trained language modelsâ€™ understanding of distributivity via causal mediation
[2] Finlayson, M., et al. What makes instruction learning hard? an investigation and a new challenge in a
[3] Khot, T., et al. Decomposed prompting: A modular approach for solving complex tasks. ArXiv,
[4] Lake, B. M. and Baroni, M. Generalization without systematicity: On the compositional skills of
[5] Merrill, W. and Sabharwal, A. Log-precision transformers are constant-depth uniform threshold circuits.
[6] Mishra, S., et al. Cross-task generalization via natural language crowdsourcing instructions. In ACL.
[7] Wei, J., et al. Chain of thought prompting elicits reasoning in large language models.
[8] Wei, J., et al. Finetuned language models are zero-shot learners. In ICLR. 2022.
