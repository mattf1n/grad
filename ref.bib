@inproceedings{ravfogel-etal-2021-counterfactual,
    title = "Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction",
    author = "Ravfogel, Shauli  and
      Prasad, Grusha  and
      Linzen, Tal  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.conll-1.15",
    doi = "10.18653/v1/2021.conll-1.15",
    pages = "194--209",
    abstract = "When language models process syntactically complex sentences, do they use their representations of syntax in a manner that is consistent with the grammar of the language? We propose AlterRep, an intervention-based method to address this question. For any linguistic feature of a given sentence, AlterRep generates counterfactual representations by altering how the feature is encoded, while leaving in- tact all other aspects of the original representation. By measuring the change in a model{'}s word prediction behavior when these counterfactual representations are substituted for the original ones, we can draw conclusions about the causal effect of the linguistic feature in question on the model{'}s behavior. We apply this method to study how BERT models of different sizes process relative clauses (RCs). We find that BERT variants use RC boundary information during word prediction in a manner that is consistent with the rules of English grammar; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category.",
}

@article{Ban2022TestingPL,
  title={Testing Pre-trained Language Models' Understanding of Distributivity via Causal Mediation Analysis},
  author={Pangbo Ban and Yifan Jiang and Tianran Liu and Shane Steinert-Threlkeld},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.04761}
}

@inproceedings{finlayson-etal-2021-causal,
    title = "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models",
    author = "Finlayson, Matthew  and
      Mueller, Aaron  and
      Gehrmann, Sebastian  and
      Shieber, Stuart  and
      Linzen, Tal  and
      Belinkov, Yonatan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.144",
    doi = "10.18653/v1/2021.acl-long.144",
    pages = "1828--1843",
    abstract = "Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. We investigate the magnitude of models{'} preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. We uncover similarities and differences across architectures and model sizes{---}notably, that larger models do not necessarily learn stronger preferences. We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure.",
}

@article{Finlayson2022WhatMI,
  title={What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment},
  author={Matthew Finlayson and Kyle Richardson and Ashish Sabharwal and Peter Clark},
  journal={EMNLP},
  year={2022},
  volume={abs/2204.09148}
}

@article{Mishra2022LilaAU,
  title={Lila: A Unified Benchmark for Mathematical Reasoning},
  author={Swaroop Mishra and Matthew Finlayson and Pan Lu and Leonard Tang and Sean Welleck and Chitta Baral and Tanmay Rajpurohit and Oyvind Tafjord and Ashish Sabharwal and Peter Clark and A. Kalyan},
  journal={EMNLP},
  year={2022},
  volume={abs/2210.17517}
}

@article{Merrill2022LogPrecisionTA,
  title={Log-Precision Transformers are Constant-Depth Uniform Threshold Circuits},
  author={William Merrill and Ashish Sabharwal},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.00729}
}

@article{Hahn2020TheoreticalLO,
  title={Theoretical Limitations of Self-Attention in Neural Sequence Models},
  author={Michael Hahn},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  volume={8},
  pages={156-171}
}

@inproceedings{Bogin2022UnobservedLS,
  title={Unobserved Local Structures Make Compositional Generalization Hard},
  author={Ben Bogin and Shivanshu Gupta and Jonathan Berant},
  year={2022}
}

@inproceedings{Dobreva2021InvestigatingNI,
  title={Investigating Negation in Pre-trained Vision-and-language Models},
  author={Radina Dobreva and Frank Keller},
  booktitle={BLACKBOXNLP},
  url={https://www.semanticscholar.org/paper/Investigating-Negation-in-Pre-trained-Models-Dobreva-Keller/0996f1d395a5226dcc38fe0e58c7b7f9433e29e5},
  year={2021}
}

@inproceedings{mishra2021crosstask,
      title={Cross-Task Generalization via Natural Language Crowdsourcing Instructions}, 
      author={Swaroop Mishra and Daniel Khashabi and Chitta Baral and Hannaneh Hajishirzi},
      year={2022},
      booktitle={ACL},
      url={https://arxiv.org/abs/2104.08773}
}

@inproceedings{
Wei2021FinetunedLM,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={ICLR},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR},   
annote={Google researchers build FLAN: an instruction following fine-tuned LM},
}

@inproceedings{Lake2018GeneralizationWS,
  title={Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author={Brenden M. Lake and Marco Baroni},
  booktitle={ICML},
  year={2018}
}

@article{Khot2022DecomposedPA,
  title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks},
  author={Tushar Khot and H. Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.02406}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{ min2022rethinking,
    title={ Rethinking the Role of Demonstrations: What makes In-context Learning Work? },
    author={ Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke },
    booktitle={ EMNLP },
    year={ 2022 }
}


@inproceedings{gao-etal-2021-making,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.295",
    doi = "10.18653/v1/2021.acl-long.295",
    pages = "3816--3830",
    abstract = "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF{---}better few-shot fine-tuning of language models{---}a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30{\%} absolute improvement, and 11{\%} on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
}
https://aclanthology.org/2021.acl-long.295.pdf
~/papers/Gao2021MakingPL.pdf

@inproceedings{shin-etal-2020-autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
    abstract = "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
}
~/papers/AutoPrompt.pdf

@inproceedings{Wallace2019UniversalAT,
  title={Universal Adversarial Triggers for Attacking and Analyzing NLP},
  author={Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
  booktitle={EMNLP},
  year={2019}
}
https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-Attacking-and-Wallace-Feng/18a1c21f35153c45d0ef30c564bffb7d70a13ccc
~/papers/Wallace2019UniversalAT

@misc{Willison2022PromptIA,
  title={Prompt injection attacks against GPT-3},
  author={Simon Willision},
  year=2022,
  url={https://simonwillison.net/2022/Sep/12/prompt-injection/},
  urldate={October 17, 2022},
}

@inproceedings{xie2022an,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RdJVFCHjUMI}
}

@inproceedings{khashabi-etal-2022-prompt,
    title = "Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts",
    author = "Khashabi, Daniel  and
      Lyu, Xinxi  and
      Min, Sewon  and
      Qin, Lianhui  and
      Richardson, Kyle  and
      Welleck, Sean  and
      Hajishirzi, Hannaneh  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Singh, Sameer  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.266",
    doi = "10.18653/v1/2022.naacl-main.266",
    pages = "3631--3643",
    abstract = "Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a {``}wayward{''} behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task), while being within a very small (2{\%}) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of various parameters. For instance, for larger model sizes we observe higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.",
}

@article{Fedus2021SwitchTS,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={William Fedus and Barret Zoph and Noam M. Shazeer},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.03961}
}

@article{Wei2022ChainOT,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed Chi and Quoc Le and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.11903}
}

@article{Pearl2001DirectAI,
  title={Direct and Indirect Effects},
  author={Judea Pearl},
  journal={Probabilistic and Causal Inference},
  year={2001}
}

@inproceedings{Vig2020InvestigatingGB,
  title={Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  author={Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and Daniel Nevo and Yaron Singer and Stuart M. Shieber},
  booktitle={NeurIPS},
  year={2020}
}

@article{Wies2022SubTaskDE,
  title={Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks},
  author={Noam Wies and Yoav Levine and Amnon Shashua},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02892}
}

@inproceedings{Salomaa1981JewelsOF,
  title={Jewels of formal language theory},
  author={Arto Salomaa},
  year={1981},
  pages={53},
}
